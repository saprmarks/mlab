{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2d4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnJVvKP4a5zPqsPB4WMhSE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW1j4HxI7kYN",
        "outputId": "05bc718a-e1fd-4aea-8227-5c2f3e817ec4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtyping in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: typeguard>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from torchtyping) (2.13.3)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchtyping) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->torchtyping) (4.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/mlab/days/w2d4\n"
          ]
        }
      ],
      "source": [
        "# if running on Google colab\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install einops\n",
        "!pip install torchtyping\n",
        "!pip install unidecode\n",
        "import torch as t\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/mlab/days/w2d4\n",
        "\n",
        "# NOTE: I had to modify the tokenizer.py file for this to work with my setup\n",
        "import days.w2d4.tokenizer as tok_tests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 0: string.split()"
      ],
      "metadata": {
        "id": "teQwfnut8KaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def corpus_common_tokens(strs, topk=30000):\n",
        "  token_counts = {}\n",
        "  for s in strs:\n",
        "    tokens = re.findall(r\"\\w+|[^\\w\\s]\", s)\n",
        "    for t in tokens:\n",
        "      if t not in token_counts: token_counts[t] = 1\n",
        "      else: token_counts[t] += 1\n",
        "  return sorted(token_counts, key=token_counts.get, reverse=True)[:topk]\n",
        "\n",
        "tok_tests.test_tokenizer_from_corpus_fn(corpus_common_tokens)"
      ],
      "metadata": {
        "id": "FFFyNuX0-XB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, token_list):\n",
        "    self.token_list = token_list\n",
        "    self.token_by_piece = {x['piece'] : x for x in token_list}\n",
        "    self.token_by_id    = {x['id'] :    x for x in token_list}\n",
        "    self.UNK_ID = 3\n",
        "\n",
        "  def decode(self, ids):\n",
        "    out = \"\"\n",
        "    for id in ids:\n",
        "      out += self.token_by_id[id]['piece']\n",
        "    return out\n",
        "\n",
        "  def tokenize(self, string):\n",
        "    pieces = re.findall(r\"\\w+|[^\\w\\s]\", string)\n",
        "    return [self.token_by_piece[p]['id'] if p in self.token_by_piece\n",
        "            else self.UNK_ID \n",
        "            for p in pieces]\n",
        "\n",
        "tok_tests.test_tokenizer(Tokenizer)"
      ],
      "metadata": {
        "id": "KRtk2Kq__tCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: BPE"
      ],
      "metadata": {
        "id": "YSymKbqDEow2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "7C3bBdzhcB2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('bpe_tokens.json') as f:\n",
        "  token_list = json.load(f)"
      ],
      "metadata": {
        "id": "C6dWJqxgKZOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BPETokenizer(Tokenizer):\n",
        "  def tokenize(self, string):\n",
        "    pcs = list(string)\n",
        "    for tok in self.token_list:\n",
        "      pc = tok['piece']\n",
        "      i = 0\n",
        "      while i < len(pcs) - 1:\n",
        "        if pcs[i] + pcs[i+1] == pc:\n",
        "          pcs[i] = pc\n",
        "          pcs.pop(i+1)\n",
        "        else:\n",
        "          i += 1\n",
        "    return [self.token_by_piece[p]['id'] if p in self.token_by_piece \n",
        "            else self.UNK_ID \n",
        "            for p in pcs]\n",
        "\n",
        "  def from_corpus(corpus, vocab_size=1000):\n",
        "    corpus = [list(text) for text in corpus]\n",
        "    # add characters to token list\n",
        "    token_list = list(set([c for text in corpus for c in text]))\n",
        "\n",
        "    # add new tokens until arriving at desired vocab size\n",
        "    while len(token_list) < vocab_size:\n",
        "      # count byte pairs\n",
        "      byte_pairs = defaultdict(lambda: 0)\n",
        "      for text in corpus:\n",
        "        for i in range(len(text) - 1):\n",
        "          byte_pairs[(text[i], text[i+1])] += 1\n",
        "      bp = max(byte_pairs.items(), key=lambda x: x[1])[0] # most common bp\n",
        "      new_token = bp[0] + bp[1]\n",
        "      token_list.append(new_token)\n",
        "      # replace occurances of bp with new token\n",
        "      for text in corpus:\n",
        "        i = 0\n",
        "        while i < len(text) - 1:\n",
        "          if (text[i], text[i+1]) == bp:\n",
        "            text[i] = new_token\n",
        "            text.pop(i+1)\n",
        "          i += 1\n",
        "      \n",
        "    token_list = [{'piece': t, 'id': i+4} for i,t in enumerate(token_list)]\n",
        "    return BPETokenizer(token_list)"
      ],
      "metadata": {
        "id": "xyEf0r1DD6Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BPETokenizer(token_list)\n",
        "token_ids = tokenizer.tokenize(\"Hello my name is Sam.\")\n",
        "print(token_ids)\n",
        "print(tokenizer.decode(token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMd8mJp2NBJY",
        "outputId": "51b71d66-69c4-4d43-8128-2520240b326d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 616, 1438, 318, 3409, 764]\n",
            "Hello my name is Sam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('shakespeare.txt', 'r') as f:\n",
        "  corpus = f.readlines()\n",
        "corpus = corpus[:100]\n",
        "\n",
        "tokenizer = BPETokenizer.from_corpus(corpus, vocab_size=1000)\n",
        "soln_tokenizer = tok_tests.BPETokenizer.from_corpus(corpus)"
      ],
      "metadata": {
        "id": "ZROCq152Y2Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"If I were a rich man I would eat a palm tree.\"\n",
        "soln_tokenizer = tok_tests.BPETokenizer.from_corpus(corpus)\n",
        "token_ids = tokenizer.tokenize(test)\n",
        "soln_token_ids = soln_tokenizer.tokenize(test)\n",
        "for t1, t2 in zip(token_ids, soln_token_ids):\n",
        "  s1, s2 = tokenizer.decode([t1]), soln_tokenizer.decode([t2])\n",
        "  if s1 != s2: print(s1, s2)"
      ],
      "metadata": {
        "id": "OAPraK7FeeVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [t['piece'] for t in tokenizer.token_list]\n",
        "soln_tokens = [t['piece'] for t in soln_tokenizer.token_list]\n",
        "for p in tokens:\n",
        "  if p not in soln_tokens: \n",
        "    print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EgtP2CzlO6z",
        "outputId": "b8be144a-0c5d-4139-988a-22034361529b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE \n",
            "IT\n",
            "IONS \n",
            "ION \n",
            "INE \n",
            "COPI\n",
            "COPIES \n",
            "ITIONS \n",
            "AT\n",
            "<<THIS ELECTRONIC VERS\n",
            "<<THIS ELECTRONIC VERSION OF THE \n",
            "<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS \n",
            "PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENED\n",
            "PROVIDED BY PROJECT GUTENBERG ETEXT OF ILLINOIS BENEDICTINE \n",
            "WITH PERMISSION.  ELECTRONIC AND MACHINE READABLE \n",
            "OF THE FUTURE COND\n",
            "OF THE FUTURE CONDITIONS \n",
            "OF THE FUTURE CONDITIONS OF THIS PRESENTAT\n",
            "TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE AL\n",
            "TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALL\n",
            "TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOW\n",
            "TO GIVE IT AWAY TO ANYONE YOU LIKE, BUT NO CHARGES ARE ALLOWED!!\n",
            "\n",
            "**W\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My from_corpus doesn't match the solution, but mine looks better (e.g. it learns that \"THE \" should be a token) and I'm more confident the code is error free."
      ],
      "metadata": {
        "id": "gGnqliKx0RZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_tests.test_tokenizer_from_corpus(BPETokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "IT-BypjRgqNU",
        "outputId": "3c4e68dd-c1d1-4c8d-d6ef-6149f0340319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-fd60ef4bff00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok_tests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_tokenizer_from_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBPETokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/gdrive/MyDrive/mlab/days/w2d4/tokenizer.py\u001b[0m in \u001b[0;36mtest_tokenizer_from_corpus\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0myours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminicorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     assert tuple([x[\"piece\"] for x in reference.token_list]) == tuple(\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"piece\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0myours\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9aYf9zH2puyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}