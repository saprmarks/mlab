{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "w2d3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObYAc/6dBi3kZ6CUgTR2L0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO0MJ3hT511w",
        "outputId": "9e356c8d-94c9-484b-9c78-4542a07a29dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.4.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtyping\n",
            "  Downloading torchtyping-0.1.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchtyping) (1.11.0+cu113)\n",
            "Collecting typeguard>=2.11.1\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->torchtyping) (4.2.0)\n",
            "Installing collected packages: typeguard, torchtyping\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 2.7.1\n",
            "    Uninstalling typeguard-2.7.1:\n",
            "      Successfully uninstalled typeguard-2.7.1\n",
            "Successfully installed torchtyping-0.1.4 typeguard-2.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/mlab/days/w2d3\n"
          ]
        }
      ],
      "source": [
        "# if running on Google colab\n",
        "!pip install einops\n",
        "!pip install torchtyping\n",
        "!pip install transformers\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "from torch import einsum\n",
        "from einops import rearrange, repeat, reduce\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/mlab/days/w2d3\n",
        "import gpt_tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Making the GPT-2 module"
      ],
      "metadata": {
        "id": "pRAGsJPr89sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedSelfAttention(nn.Module):\n",
        "  def __init__(self, hidden_size, num_heads):\n",
        "    super().__init__()\n",
        "    self.head_size = hidden_size // num_heads\n",
        "    self.attention = nn.Linear(hidden_size, 3 * num_heads * self.head_size) # output is concatenated query, key, value\n",
        "    self.project = nn.Linear(hidden_size, hidden_size) # things will break if num_heads doesn't evenly divide hidden_size\n",
        "\n",
        "  def forward(self, input, past_key_values=None, return_key_values=False):\n",
        "    # calculate raw attention scores\n",
        "    q, k, v = rearrange(self.attention(input), 'b sl (qkv nh hs) -> qkv b nh sl hs', qkv=3, hs=self.head_size)\n",
        "    if return_key_values: \n",
        "      new_kv = t.cat((k,v), dim=-1)\n",
        "      kv_cache = t.cat((past_key_values.unsqueeze(0), new_kv), dim=-2)\n",
        "      k, v = rearrange(kv_cache, 'b nh sl (kv hs) -> kv b nh sl hs', kv=2)\n",
        "    attn_pattern = einsum('bhqi,bhki->bhqk', q, k) / math.sqrt(self.head_size)\n",
        "\n",
        "    # mask the attention pattern so tokens only attend to past tokens\n",
        "    if not return_key_values: # not needed if given past_key_values\n",
        "      seq_len = attn_pattern.size(-1)\n",
        "      masked_indices = t.ones(seq_len, seq_len, device=input.device).triu(1) > 0\n",
        "      attn_pattern[...,masked_indices] = -1e4\n",
        "\n",
        "    attn_scores = attn_pattern.softmax(-1)\n",
        "    attn = einsum('bhqk,bhki->bhqi',attn_scores, v)\n",
        "    out = self.project(rearrange(attn, 'b nh sl hs -> b sl (nh hs)'))\n",
        "    if return_key_values:\n",
        "      return out, new_kv\n",
        "    else:\n",
        "      return out\n",
        "\n",
        "gpt_tests.test_unidirectional_attn(MultiHeadedSelfAttention)\n",
        "gpt_tests.test_attn_cache(MultiHeadedSelfAttention)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-YAZVS588oZ",
        "outputId": "15a23123-a643-4c78-bfb2-278e798f244e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congrats! You've passed the test!\n",
            "Checking encoding:\n",
            "Congrats! You've passed the test!\n",
            "Checking new key and value:\n",
            "Congrats! You've passed the test!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, hidden_size, num_heads, dropout=0., layer_norm_epsilon=1e-5):\n",
        "    super().__init__()\n",
        "    self.ln = nn.LayerNorm(hidden_size, layer_norm_epsilon)\n",
        "    self.attention = MultiHeadedSelfAttention(hidden_size, num_heads)\n",
        "    self.mlp = nn.Sequential(\n",
        "      nn.LayerNorm(hidden_size, layer_norm_epsilon),\n",
        "      nn.Linear(hidden_size, 4 * hidden_size),\n",
        "      nn.GELU(),\n",
        "      nn.Linear(4 * hidden_size, hidden_size),\n",
        "      nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "  def forward(self, input, past_key_values=None, return_key_values=False):\n",
        "    attn_out = self.attention(self.ln(input), past_key_values, return_key_values)\n",
        "    if return_key_values:\n",
        "      attn_out, new_key_values = attn_out \n",
        "      return self.mlp(input + attn_out) + input + attn_out, new_key_values\n",
        "    else:\n",
        "      return self.mlp(input + attn_out) + input + attn_out\n",
        "\n",
        "gpt_tests.test_gpt_block(Block)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydEvoa7SCGOl",
        "outputId": "a236cbd1-1aab-4ac3-9255-0a6af3f99755"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Congrats! You've passed the test!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from torchtyping import TensorType\n",
        "\n",
        "@dataclass\n",
        "class GPT2Output:\n",
        "    logits: TensorType[\"batch_size\", \"vocab_size\"]\n",
        "    final_encoding: TensorType[\"batch_size\", \"hidden_size\"]"
      ],
      "metadata": {
        "id": "rw6uOdx7MnY3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size, max_position_embeddings):\n",
        "    super().__init__()\n",
        "    self.token_embed = nn.Parameter(t.randn(vocab_size, hidden_size))\n",
        "    self.pos_embed   = nn.Parameter(t.randn(max_position_embeddings, hidden_size))\n",
        "\n",
        "  def forward(self, input):\n",
        "    seq_len = input.size(-1)\n",
        "    pos = t.arange(0, seq_len, device=input.device)\n",
        "    return self.token_embed[input] + self.pos_embed[pos]"
      ],
      "metadata": {
        "id": "MDszAj23X6yt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "  def __init__(self, num_layers, num_heads, vocab_size, hidden_size, \n",
        "               max_position_embeddings, dropout, layer_norm_epsilon, use_cache=False):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.head_size = hidden_size // num_heads\n",
        "    self.embed = Embedding(vocab_size, hidden_size, max_position_embeddings)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[Block(hidden_size, num_heads, dropout, layer_norm_epsilon) for _ in range(num_layers)]\n",
        "    )\n",
        "    self.ln = nn.LayerNorm(hidden_size)\n",
        "    self.use_cache = use_cache\n",
        "    self.kv_cache = t.zeros(num_layers, num_heads, 0, 2 * self.head_size)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.dropout(self.embed(input))\n",
        "    if self.use_cache: # remove previously processed tokens\n",
        "      prev_seq_len = self.kv_cache.size(-2)\n",
        "      x = x[...,prev_seq_len:,:]\n",
        "\n",
        "    if not self.use_cache:\n",
        "      x = self.blocks(x)\n",
        "\n",
        "    else:\n",
        "      new_kvs = []\n",
        "      for i, block in enumerate(self.blocks):\n",
        "        x, new_kv = block(x, past_key_values=self.kv_cache[i], return_key_values=self.use_cache)\n",
        "        new_kvs.append(new_kv)\n",
        "      new_kvs = t.cat(new_kvs)\n",
        "      self.kv_cache = t.cat((self.kv_cache, new_kvs), dim=-2)\n",
        "\n",
        "    final_encoding = self.ln(x)[...,-1,:]\n",
        "    logits = einsum('ij,bj->bi',self.embed.token_embed, final_encoding)\n",
        "    return GPT2Output(logits=logits, final_encoding=final_encoding)\n",
        "\n",
        "  def clear_cache(self):\n",
        "    self.kv_cache = t.zeros(self.num_layers, self.num_heads, 0, 2 * self.head_size)\n",
        "\n",
        "  def next_token(self, input_ids, temperature, freq_penalty=2.0):\n",
        "    logits = self.forward(input_ids.unsqueeze(0)).logits.squeeze(0)\n",
        "\n",
        "    # tally frequencies\n",
        "    id_frequencies = t.zeros(logits.size(0))\n",
        "    for i in range(input_ids.size(0)): id_frequencies[input_ids[i]] += 1\n",
        "\n",
        "    token_dist = (logits/temperature - id_frequencies * freq_penalty).softmax(-1)\n",
        "    return t.multinomial(token_dist, num_samples=1)\n",
        "\n",
        "  def generate(self, text, max_length=30, temperature=1.0, freq_penalty=2.0):\n",
        "    self.clear_cache()\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    input_ids = t.LongTensor(tokenizer(text).input_ids)\n",
        "    for _ in range(max_length):\n",
        "      next_token = self.next_token(input_ids, temperature, freq_penalty)\n",
        "      input_ids = t.cat((input_ids, next_token))\n",
        "      if next_token[0] == tokenizer.eos_token_id : break\n",
        "    return tokenizer.decode(input_ids)\n",
        "    \n",
        "\n",
        "gpt_tests.test_gpt(GPT2)\n",
        "gpt_tests.test_gpt_cache(GPT2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDGWCnnLcX-u",
        "outputId": "be6195a0-c366-45ad-d153-9ff59f1f8035"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking logits:\n",
            "Congrats! You've passed the test!\n",
            "Checking final encodings:\n",
            "Congrats! You've passed the test!\n",
            "Congrats! Your GPT returns the same results with and without cache.\n",
            "It took 7.406s to generate a 500-token sentence without cache and 1.176s with cache.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Loading pretrained weights"
      ],
      "metadata": {
        "id": "GWSJ4k1fk8RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5)\n",
        "pretrained_gpt = gpt_tests.get_pretrained_gpt()"
      ],
      "metadata": {
        "id": "ekQl-CfRhJpN"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dict = {}\n",
        "for (k,_), (_, v) in zip(my_gpt.state_dict().items(), pretrained_gpt.state_dict().items()):\n",
        "  load_dict[k] = v\n",
        "my_gpt.load_state_dict(load_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7uzinYplmMg",
        "outputId": "202fb30d-e8cc-4265-aee6-4464e0f8df3f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Efficient text generation"
      ],
      "metadata": {
        "id": "oaGcHpPim0m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "IwXjc3OJl_wy"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = t.LongTensor(tokenizer([\n",
        "                                 \"My life motto:\",\n",
        "                                 \"My life motto: Fortune\",\n",
        "                                 \"My life motto: Fortune favors\",\n",
        "                                 \"My life motto: Fortune favors the\",\n",
        "                                 \"My life motto: Fortune favors the bold\"\n",
        "                                ],padding=True).input_ids)"
      ],
      "metadata": {
        "id": "e8P4u419qwzU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_gpt = GPT2(num_layers=12, num_heads=12, vocab_size=50257, hidden_size=768, max_position_embeddings=1024, dropout=0.1, layer_norm_epsilon=1e-5, use_cache=True)\n",
        "load_dict = {}\n",
        "for (k,_), (_, v) in zip(my_gpt.state_dict().items(), pretrained_gpt.state_dict().items()):\n",
        "  load_dict[k] = v\n",
        "my_gpt.load_state_dict(load_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8LROH8rik6c",
        "outputId": "53c7f4fd-534f-4a06-871a-eee109a5cfd0"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_gpt.generate(\"I woke up and got out of\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UHZCG6Ahuyhs",
        "outputId": "eb4023ff-6b9f-4ac9-d05a-78d53491641e"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I woke up and got out of my bed, hit 8 liters as I went fro-up as fire swept through the bag. At 50 liters one could even first on your'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "u4K0zMN0xugZ"
      },
      "execution_count": 214,
      "outputs": []
    }
  ]
}